<!DOCTYPE HTML>
<!--
	Strata by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Qing Liu's Homepage</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<div class="inner">
					<a href="#" class="image avatar"><img src="images/avatar.jpg" alt="" /></a>
					<h1><strong>I am Qing Liu (刘晴)</strong>, a Ph.D. candidate at<br />
					Johns Hopkins University, <br />
					supervised by <a href="http://www.cs.jhu.edu/~ayuille/">Dr. Alan Yuille</a>.</h1>
				</div>
			</header>

		<!-- Main -->
			<div id="main">

				<!-- One -->
					<section id="bio">
						<header class="major">
							<h2>About me</h2>
						</header>
						<p>My main research interests are in the area of object recognition and parsing with weak supervision, including weakly supervised learning, domain adaptation (learning from synthetic), few-shot/zero-shot learning, etc.. I am going to graduate in 2021 and I will join Adobe as a research engineer.</p>
						<div class="col-6 col-6-medium col-12-small">
							<section class="box style1">
								<h3 class="icon solid fa-user-graduate"> Education</h3>
									<ul>
										<li>
											<b>Ph.D. </b> in Computer Science, Johns Hopkins University 
										</li>
										<li>
											<b>M.S.E.</b> in Computer Science, Johns Hopkins University 
										</li>
										<li>
											<b>M.S.</b> in Molecular and Cell Biology, The University of Texas at Dallas
										</li>
										<li>
											<b>B.S.</b> in Chemistry and Psychology , Peking University
										</li>
									</ul>
							</section>
						</div>
						<ul class="actions">
							<li><a href="#publication" class="button">Publication</a></li>
							<li><a href="#Projects" class="button">Projects</a></li>
							<li><a href="#Experience" class="button">Experience</a></li>
						</ul>
					</section>

				<section id="publication">
						<header class="major">
							<h2>Publication</h2>
						</header>
						<ul>
							<li>
								<b>Qing Liu</b>, Adam Kortylewski, Zhishuai Zhang, Zizhang Li, Mengqi Guo, Qihao Liu, Xiaoding Yuan, Jiteng Mu, Weichao Qiu, Alan Yuille.
								<b>CGPart: A Part Segmentation Dataset Based on 3D Computer Graphics Models.</b>
								<br>
								[<a href="https://arxiv.org/pdf/2103.14098.pdf" target="_blank">arXiv</a>]
								[<a href="https://qliu24.github.io/cgpart/" target="_blank">Dataset</a>]
								<!-- [<a href="" target="_blank">Code</a>]
								[<a href="" target="_blank">Slides</a>]
								[<a href="" target="_blank">Poster</a>] -->
								<br><br>
							</li>
							<li>
								<b>Qing Liu</b>, Vignesh Ramanathan, Dhruv Mahajan, Alan Yuille, Zhenheng Yang.  
								<b>Weakly Supervised Instance Segmentation for Videos with Temporal Mask Consistency.</b>
								<br> Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
								<br>
								[<a href="https://arxiv.org/pdf/2103.12886.pdf" target="_blank">arXiv</a>]
								[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Weakly_Supervised_Instance_Segmentation_for_Videos_With_Temporal_Mask_Consistency_CVPR_2021_paper.pdf" target="_blank">Paper</a>]
								<!-- [<a href="" target="_blank">Code</a>]-->
								[<a href="poster_slides/CVPR-5min-slides.pdf" target="_blank">Slides</a>]
								[<a href="poster_slides/cvpr21_poster_wsis.pdf" target="_blank">Poster</a>] 
								<br><br>
							</li>
							<li>
								Adam Kortylewski, <b>Qing Liu</b>, Angtian Wang, Yihong Sun, Alan Yuille. 
								<b>Compositional convolutional neural networks: A robust and interpretable model for object recognition under occlusion.</b>
								<br> International Journal of Computer Vision (2020): 1-25. 
								<br>
								[<a href="https://arxiv.org/pdf/2006.15538.pdf" target="_blank">arXiv</a>]
								[<a href="https://link.springer.com/article/10.1007/s11263-020-01401-3" target="_blank">Paper</a>]
								<!-- [<a href="" target="_blank">Code</a>]
								[<a href="" target="_blank">Slides</a>]
								[<a href="" target="_blank">Poster</a>] -->
								<br><br>
							</li>
							<li>
								<b>Qing Liu</b>, Orchid Majumder, Alessandro Achille, Avinash Ravichandran, Rahul Bhotika, Stefano Soatto. 
								<b>Incremental Meta-Learning via Indirect Discriminant Alignment.</b>
								<br> Proceedings of the European Conference on Computer Vision. 2020. <br>
								[<a href="https://arxiv.org/pdf/2002.04162.pdf" target="_blank">arXiv</a>]
								[<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520664.pdf" target="_blank">Paper</a>]
								<!-- [<a href="" target="_blank">Code</a>]
								[<a href="" target="_blank">Poster</a>] -->
								[<a href="poster_slides/eccv20_long_video.pdf" target="_blank">Slides</a>]
								<br><br>
							</li>
							<li>
								Adam Kortylewski, Ju He, <b>Qing Liu</b>, Alan Yuille. 
								<b>Compositional Convolutional Neural Networks: A Deep Architecture with Innate Robustness to Partial Occlusion.</b>
								<br> Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
								<br>
								[<a href="https://arxiv.org/pdf/2003.04490.pdf" target="_blank">arXiv</a>]
								[<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Kortylewski_Compositional_Convolutional_Neural_Networks_A_Deep_Architecture_With_Innate_Robustness_CVPR_2020_paper.pdf" target="_blank">Paper</a>]
								[<a href="https://github.com/AdamKortylewski/CompositionalNets" target="_blank">Code</a>]
								<!-- [<a href="" target="_blank">Slides</a>]
								[<a href="" target="_blank">Poster</a>] -->
								<br><br>
							</li>
							<li>
								Adam Kortylewski, <b>Qing Liu</b>, Huiyu Wang, Zhishuai Zhang, Alan Yuille. 
								<b>Combining Compositional Models and Deep Networks For Robust Object Classification under Occlusion.</b>
								<br> Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2020.
								<br>
								[<a href="https://arxiv.org/pdf/1905.11826.pdf" target="_blank">arXiv</a>]
								[<a href="https://openaccess.thecvf.com/content_WACV_2020/papers/Kortylewski_Combining_Compositional_Models_and_Deep_Networks_For_Robust_Object_Classification_WACV_2020_paper.pdf" target="_blank">Paper</a>]
								[<a href="https://github.com/AdamKortylewski/CompositionalNets" target="_blank">Code</a>]
								<!-- [<a href="" target="_blank">Slides</a>]
								[<a href="" target="_blank">Poster</a>] -->
								<br><br>
							</li>
							<li>
								<b>Qing Liu</b>, Lingxi Xie, Huiyu Wang, Alan Yuille.
								<b>Semantic-Aware Knowledge Preservation for Zero-Shot Sketch-Based Image Retrieval.</b>
								<br>Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019. <br>
								[<a href="https://arxiv.org/pdf/1904.03208.pdf" target="_blank">arXiv</a>]
								[<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Semantic-Aware_Knowledge_Preservation_for_Zero-Shot_Sketch-Based_Image_Retrieval_ICCV_2019_paper.pdf" target="_blank">Paper</a>]
								[<a href="https://github.com/qliu24/SAKE" target="_blank">Code</a>]
								[<a href="poster_slides/ZSSBIR_Poster-2.pdf" target="_blank">Poster</a>]
								<br><br>
							</li>
							<li>
								<b>Qing Liu</b>*, Yutong Bai*, Lingxi Xie, Yan Zheng, Weichao Qiu, Alan Yuille.
								<b>Semantic Part Detection via Matching: Learning to Generalize to Novel Viewpoints from Limited Training Data.</b>
								<br>Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019. <br>
								[<a href="https://arxiv.org/pdf/1811.11823.pdf" target="_blank">arXiv</a>]
								[<a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Bai_Semantic_Part_Detection_via_Matching_Learning_to_Generalize_to_Novel_ICCV_2019_paper.pdf" target="_blank">Paper</a>]
								[<a href="https://github.com/ytongbai/SemanticPartDetection" target="_blank">Code</a>]
								<!-- [<a href="" target="_blank">Slides</a>]
								[<a href="" target="_blank">Poster</a>] -->
								<br><br>
							</li>
							<li>
								Adam Kortylewski, <b>Qing Liu</b>, Huiyu Wang, Zhishuai Zhang, Alan Yuille. 
								<b>Localizing Occluders with Compositional Convolutional Networks. </b>
								<br> Proceedings of the IEEE/CVF International Conference on Computer Vision Workshop. 2019.
								<br>
								[<a href="https://arxiv.org/pdf/1911.08571.pdf" target="_blank">arXiv</a>]
								[<a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/NeurArch/Kortylewski_Localizing_Occluders_with_Compositional_Convolutional_Networks_ICCVW_2019_paper.pdf" target="_blank">Paper</a>]
								<!-- [<a href="" target="_blank">Code</a>]
								[<a href="" target="_blank">Slides</a>]
								[<a href="" target="_blank">Poster</a>] -->
								<br><br>
							</li>
							<li>
								Hongjing Lu, <b>Qing Liu</b>, Nicholas Ichien, Alan L. Yuille, Keith J. Holyoak. 
								<b>Seeing the Meaning: Vision meets Semantics in Solving Pictorial Analogy Problems.</b>
								<br>Proceedings of the 41st Annual Meeting of the Cognitive Science Society. 2019.
								<br>
								[<a href="https://par.nsf.gov/servlets/purl/10093529" target="_blank">Paper</a>]
								<!-- [<a href="" target="_blank">Dataset</a>]
								[<a href="" target="_blank">Code</a>]
								[<a href="" target="_blank">Slides</a>]
								[<a href="" target="_blank">Poster</a>] -->
								<br><br>
							</li>
							<li>
								Boyang Deng, <b>Qing Liu</b>, Siyuan Qiao, Alan Yuille. 
								<b>Few-shot Learning by Exploiting Visual Concepts within CNNs.</b>
								<br>
								[<a href="https://arxiv.org/pdf/1711.08277.pdf" target="_blank">arXiv</a>]
								<!-- [<a href="" target="_blank">Dataset</a>]
								[<a href="" target="_blank">Code</a>]
								[<a href="" target="_blank">Slides</a>]
								[<a href="" target="_blank">Poster</a>] -->
								<br><br>
							</li>
						</ul>
					</section>

				<!-- Two -->
				<!--
					<section id="two">
						<h2>Recent Work</h2>
						<div class="row">
							<article class="col-6 col-12-xsmall work-item">
								<a href="images/fulls/01.jpg" class="image fit thumb"><img src="images/thumbs/01.jpg" alt="" /></a>
								<h3>Magna sed consequat tempus</h3>
								<p>Lorem ipsum dolor sit amet nisl sed nullam feugiat.</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="images/fulls/02.jpg" class="image fit thumb"><img src="images/thumbs/02.jpg" alt="" /></a>
								<h3>Ultricies lacinia interdum</h3>
								<p>Lorem ipsum dolor sit amet nisl sed nullam feugiat.</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="images/fulls/03.jpg" class="image fit thumb"><img src="images/thumbs/03.jpg" alt="" /></a>
								<h3>Tortor metus commodo</h3>
								<p>Lorem ipsum dolor sit amet nisl sed nullam feugiat.</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="images/fulls/04.jpg" class="image fit thumb"><img src="images/thumbs/04.jpg" alt="" /></a>
								<h3>Quam neque phasellus</h3>
								<p>Lorem ipsum dolor sit amet nisl sed nullam feugiat.</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="images/fulls/05.jpg" class="image fit thumb"><img src="images/thumbs/05.jpg" alt="" /></a>
								<h3>Nunc enim commodo aliquet</h3>
								<p>Lorem ipsum dolor sit amet nisl sed nullam feugiat.</p>
							</article>
							<article class="col-6 col-12-xsmall work-item">
								<a href="images/fulls/06.jpg" class="image fit thumb"><img src="images/thumbs/06.jpg" alt="" /></a>
								<h3>Risus ornare lacinia</h3>
								<p>Lorem ipsum dolor sit amet nisl sed nullam feugiat.</p>
							</article>
						</div>
						<ul class="actions">
							<li><a href="#" class="button">Full Portfolio</a></li>
						</ul>
					</section>
				-->


				

		<section id="Projects" class="wrapper style3">
				<header>
					<h2>Recent Projects</h2>
				</header>
				<article id="cgpart" class="wrapper style2">
					<div class="container medium">
						<header><h3>CGPart: A Part Segmentation Dataset Based on 3D Computer Graphics Models</h3></header>
						<article class="col-6 col-12-xsmall work-item">
								<img src="projects/cgpart/cgpart_car.gif" alt="demo_car_cruiser" width="270"/><img src="projects/cgpart/cgpart_jet.gif" alt="demo_car_cruiser" width="270"/>
								<h5>Part segmentations provide a rich and detailed part-level description of objects, but their annotation requires an enormous amount of work. In this paper, we introduce CGPart, a comprehensive part segmentation dataset that provides detailed annotations on 3D CAD models, synthetic images, and real test images. To illustrate the value of CGPart, we apply it to image part segmentation through unsupervised domain adaptation (UDA). We evaluate several baseline methods by adapting top-performing UDA algorithms from related tasks to part segmentation. Moreover, we introduce a new method called Geometric-Matching Guided domain adaptation (GMG), which leverages the spatial object structure to guide the knowledge transfer from the synthetic to the real images. Experimental results demonstrate the advantage of our algorithm and reveal insights for future improvement.</h5>
						</article>
					</div>
				</article>
				<article id="weakly" class="wrapper style2">
					<div class="container medium">
						<header><h3>Weakly Supervised Instance Segmentation for Videos with Temporal Mask Consistency</h3></header>
						<article class="col-6 col-12-xsmall work-item">
								<img src="projects/weakly/weakly.png" alt="demo" width="540"/>
								<h5>Weakly supervised instance segmentation reduces the cost of annotations required to train models. However, existing approaches which rely only on image-level class labels predominantly suffer from errors due to (a) partial segmentation of objects and (b) missing object predictions. We show that these issues can be better addressed by training with weakly labeled videos instead of images, and are the first to explore the use of video signals to tackle weakly supervised instance segmentation. First, we adapt inter-pixel relation network (IRN) to effectively incorporate motion information during training. Second, we introduce a new MaskConsist module, which addresses the problem of missing object instances by transferring stable predictions between neighboring frames during training. We demonstrate that both approaches together improve the instance segmentation metric AP50 on video frames of two datasets: Youtube-VIS and Cityscapes by 5% and 3% respectively.</h5>
						</article>
					</div>
				</article>
				<article id="ida" class="wrapper style2">
					<div class="container medium">
						<header><h3>Incremental Few-Shot Meta-Learning via Indirect Discriminant Alignment</h3></header>
						<article class="col-6 col-12-xsmall work-item">
								<img src="projects/ida/IDA.png" alt="demo" width="540"/>
								<h5>We propose a method to train a model so it can learn new classification tasks while improving with each task solved. This amounts to combining meta-learning with incremental learning. Different tasks can have disjoint classes, so one cannot directly align different classifiers as done in model distillation. On the other hand, simply aligning features shared by all classes does not allow the base model sufficient flexibility to evolve to solve new tasks. We therefore indirectly align features  relative to a minimal set of "anchor classes". Such <em>indirect discriminant alignment</em> (IDA) adapts a new model to old classes without the need to re-process old data, while leaving  maximum flexibility for the model to adapt to new tasks. This process enables incrementally improving the model by processing multiple learning <em>episodes</em>, each representing a different learning task, even with few training examples. Experiments on few-shot learning benchmarks show that this incremental approach performs favorably compared to training the model with the entire dataset at once.</h5>
						</article>
					</div>
				</article>
				<article id="sketch" class="wrapper style2">
					<div class="container medium">
						<header><h3>Semantic-Aware Knowledge Preservation for Zero-Shot Sketch-Based Image Retrieval</h3></header>
						<article class="col-6 col-12-xsmall work-item">
								<img src="projects/sake/sake.png" alt="demo" width="540"/>
								<h5>We investigate the problem ZS-SBIR from the viewpoint of domain adaptation. Based on a framework which starts with a pre-trained model on ImageNet and fine-tunes it on the training set of SBIR benchmark, we advocate the importance of preserving previously acquired knowledge, e.g., the rich discriminative features learned from ImageNet, so as to improve the model's transfer ability. Zero-shot experiments on two extended SBIR datasets verify the superior performance of our approach. Extensive diagnostic experiments validate that knowledge preserved benefits SBIR in zero-shot settings, as a large fraction of the performance gain is from the more properly structured feature embedding for photo images.</h5>
						</article>
					</div>
				</article>
		</section>


		<section id="Experience" class="wrapper style3">
				<header>
					<h2>Experience</h2>
				</header>
				<article id="services" class="wrapper style2">
					<div class="container medium">
						<header><h2>Academic Services</h2></header>
						<ul>
							<li>
								<b>Reviewer</b>:
								CVPR 2022, AAAI 2022, WACV 2022, CVPR 2021, ICCV 2021, WACV 2021, ICCV 2019
							</li>
							<li>
								<b>Teaching Assistant</b>:
								EN. 601.461/661: Computer Vision. <br>
								Johns Hopkins University - Spring 2020
							</li>
						</ul>
					</div>
				</article>
				<article id="working_experience" class="wrapper style2">
					<div class="container medium">
						<header><h2>Work Experience</h2></header>
						<ul>
							<li>
								<b>Research Intern, Facebook AI </b>:
								2020/05-2020/09
							</li>
							<li>
								<b>Applied Scientist Intern, Amazon AWS ReKognition</b>:
								2019/06-2019/09
							</li>
							<li>
								<b>Applied Scientist Intern, Amazon Transaction Risk Management Systems</b>:
								2018/05-2018/08
							</li>
						</ul>
					</div>
				</article>
		</section>


		<!-- Footer -->
			<footer id="footer">
				<div class="inner">
					<ul class="icons">
						<li><a href="https://scholar.google.com/citations?user=1ytghtEAAAAJ&hl=en" class="icon solid fa-graduation-cap"><span class="label">Google_scholar</span></a></li>
						<li><a href="https://github.com/qliu24" class="icon brands fa-github"><span class="label">Github</span></a></li>
						<!-- <li><a href="#" class="icon brands fa-dribbble"><span class="label">Dribbble</span></a></li> -->
						<li><a id="email" href="mailto:qingliu@jhu.edu" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
						<li><a href="resume/resume_qingliu.pdf" class="icon fa-id-card"><span class="label">CV</span></a></li>
					</ul>
					<ul class="copyright">
						<li>&copy; Qing Liu</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>
			<button onclick="topFunction()" id="top_btn" title="Go to top">Top</button>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.poptrox.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

			<script>
			//Get the button
			var mybutton = document.getElementById("top_btn");

			// When the user scrolls down 20px from the top of the document, show the button
			window.onscroll = function() {scrollFunction()};

			function scrollFunction() {
			  if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
			    mybutton.style.display = "block";
			  } else {
			    mybutton.style.display = "none";
			  }
			}

			// When the user clicks on the button, scroll to the top of the document
			function topFunction() {
			  document.body.scrollTop = 0;
			  document.documentElement.scrollTop = 0;
			}
			</script>

	</body>
</html>